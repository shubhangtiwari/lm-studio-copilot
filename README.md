# LM Studio Copilot VS Code Extension

A full-featured Copilot-like chat extension for VS Code, powered by your local LM Studio server (OpenAI Chat API compatible).

## âœ¨ Features
- **Chat panel** with streaming LLM responses and multi-turn memory
- **File context selection**: include files/folders as context for the LLM
- **Inline code actions**: apply code blocks from chat directly at your cursor
- **Workspace-wide refactor**: LLM can suggest and apply multi-file edits, with diff preview and approval
- **Command palette integration**: start chat or apply suggestions from anywhere
- **File operations**: read, update, and create files from chat suggestions
- **Beautiful, modern UI** with chat bubbles and suggestions

## ğŸš€ Getting Started
1. Make sure [LM Studio](https://lmstudio.ai/) is running locally at `http://localhost:1234` and exposes the OpenAI Chat Completion API.
2. Install this extension in VS Code.
3. Open the Command Palette and run `LM Studio Copilot: Start Chat`.
4. Select files for context, chat with the LLM, and apply suggestions or code blocks as needed.

## ğŸ–¥ï¸ Screenshots
<!-- Add screenshots of the chat panel, file context selection, and diff preview here -->

## âš™ï¸ Requirements
- VS Code 1.80+
- LM Studio server running locally at `http://localhost:1234` (OpenAI API compatible)

## ğŸ› ï¸ Extension Settings
_No custom settings yet. All features are available out of the box._

## ğŸ“ Known Issues
- TypeScript lint errors in `extension.ts` are expected due to embedded HTML/JS and do not affect runtime.
- Streaming requires LM Studio server to support OpenAI streaming API.

## ğŸ“¢ Release Notes
### 1.0.0
- Initial release: Copilot-like chat, file context, inline code, workspace refactor, streaming, and more!

---

## ğŸ¤ Contributing
PRs and feedback welcome! See the code for extension points.

## ğŸ“š License
MIT
